\chapter{Guiding Soft Robots with Motor-Imagery Brain Signals}
\label{chp:braincontrol}

\begin{foreword}
    % In Chapter~\ref{chp:hsacontrol}, we demonstrated how we can effectively control the motion of \gls{HSA} robots in (planar) operational space. Still, these controllers naturally require setpoints or reference trajectories, which we have so far hardcoded. Instead, we investigate in this chapter how operational space setpoints can be guided effectively by the (human) user in the framework of \gls{HRI}. Specifically, we consider here a scenario where soft robots would be used to assist the elderly or impaired people with \glsxtrfull{ADL}. Soft robots are a promising avenue for this scenario, as their passive compliance increases the safety in such closed contact scenarios with humans, as seen in Chapter~\ref{chp:safetymetric}. Here, it is important for the \gls{HRI} to be a) intuitive and effective at guiding the soft robot at the given task, b) as barrier-free and interaction-free as possible, c) to be wearable instead of being a stationary interface, and, finally, d) in case there are operation errors caused by the user and/or the \gls{HRI} for the robot to continue exhibiting a safe behavior. Wearable \gls{BMI} are a very promising avenue as they allow the user to operate the machine (in this case, the soft robot) via their thoughts without necessitating physical interactions with the interface. One prominent \gls{BMI} are \gls{EEG} devices combined with motor imagery (i.e., the user imagining motor movements). However, currently, the classification accuracies of signals stemming from wearable, few-channel \gls{EEG} devices are rather low, which can potentially lead to safety issues when the robot deviates in its motion from the user's intention. Furthermore, current classifiers are only decently accurate on binary or, at maximum, three-class classification problems, which makes it challenging to translate this into meaningful motion commands for the soft robot. As a solution, we devise in this Chapter a protocol for operating soft robots, and in particular planar \gls{HSA} robots via motor imagery. For this, we combine the proposed \gls{BMI} protocol with an impedance controller, which allows us to combine the passive compliance of soft robots with active compliance of the Cartesian-space impedance controller for safe operations even when the brain signals / the user's intent are wrongly classified.
    % chapgpt-revised version
    In Chapter~\ref{chp:hsacontrol}, we demonstrated how the motion of \gls{HSA} robots in planar operational space can be effectively controlled. However, these controllers inherently require setpoints or reference trajectories, which we have so far predefined. In this chapter, we explore how operational space setpoints can be guided more effectively by a (human) user within the framework of \gls{HRI}. Specifically, we consider a scenario where soft robots assist elderly or impaired individuals with \glsxtrfull{ADL}. Soft robots are particularly suitable for this application due to their passive compliance, which enhances safety in close-contact interactions with humans, as discussed in Chapter~\ref{chp:safetymetric}.
    %
    For such \glspl{HRI}, it is crucial that the interaction be: (a) intuitive and effective for guiding the robot in its task, (b) minimally invasive and barrier-free, (c) wearable rather than relying on stationary interfaces, and (d) capable of ensuring the robot continues to operate safely even in cases of user or system error. Wearable \gls{BMI} present a compelling solution, as they allow users to control the robot (in this case, a soft robot) through thought, eliminating the need for physical interaction with the interface. One prominent type of \gls{BMI} involves \gls{EEG} devices combined with motor imagery (i.e., the user imagining motor movements).
    %
    However, current wearable, few-channel \gls{EEG} devices face challenges, including low classification accuracies, which can compromise safety if the robot’s motion deviates from the user’s intent. Additionally, existing classifiers are generally limited to binary or, at most, three-class problems, making it difficult to generate meaningful motion commands for the robot.
    %
    To address these issues, we propose in this chapter a protocol for operating soft robots, particularly planar \gls{HSA} robots, using motor imagery. This protocol integrates the \gls{BMI} approach with an impedance controller, combining the passive compliance of soft robots with the active compliance of a Cartesian-space impedance controller. This integration ensures safe operation, even when brain signals or user intent are misclassified.
\end{foreword}

\pagebreak

\begin{abstract}
    Integrating Brain-Machine Interfaces into non-clinical applications like robot motion control remains difficult - despite remarkable advancements in clinical settings. Specifically, EEG-based motor imagery systems are still error-prone, posing safety risks when rigid robots operate near humans. This chapter presents an alternative pathway towards safe and effective operation by combining wearable EEG with physically embodied safety in soft robots. We introduce and test a pipeline that allows a user to move a soft robot's end effector in real time via brain waves that are measured by as few as three EEG channels. A robust motor imagery algorithm interprets the user's intentions to move the position of a virtual attractor to which the end effector is attracted, thanks to a Cartesian impedance controller. We specifically focus here on planar soft robot-based architected metamaterials. Due to their compact and portable nature, they could be mounted to a mobile platform in the future and allow assistance with \glsxtrfull{ADL}. We preliminarily but quantitatively evaluate the approach on the task of setpoint regulation. We observe that the user reaches the proximity of the setpoint in 66\% of steps and that for successful steps, the average response time is 21.5s. We also demonstrate the execution of simple real-world tasks involving interaction with the environment, which would be extremely hard to perform if it were not for the robot's softness.
\end{abstract}

\blfootnote{This chapter is partly based on \faFileTextO \, \faTrophy~\emph{\textbf{M. Stölzle}*, S. S. Baberwal*, D. Rus, S. Coyle$^\dagger$, and C. Della Santina$^\dagger$ (2024). Guiding Soft Robots with Motor-Imagery Brain Signals and Impedance Control. In Proceedings of the 2024 IEEE 7th International Conference on Soft Robotics (RoboSoft) (pp. 1-8). IEEE. Received the \textbf{Best Paper Award}}~\citep{stolzle2024guiding}.

% \nth{1}-author contributions: M. Stölzle and S. S. Baberwal conceived the research project, devised the protocol for translating motor imagery classification into end-effector motion, conducted all experiments, and wrote the paper together. M. Stölzle formulated and developed the Cartesian impedance controller and constructed the experimental setup (e.g., motion capture system, setpoint projection, mounting the hairspray container). S. S. Baberwal implemented the \gls{EEG} data processing pipeline, which included the motor imagery classifiers.

$^*$M.S. and S.S.B. contributed equally to this work. $^\dagger$C.D.S and S.C. shared equal supervising duties.
M.S., S.S.B., and C.D.S. conceived the research project.
M.S. and S.S.B. devised the protocol for translating motor imagery classification into end-effector motion, conducted all experiments, and wrote the paper together. 
M.S. formulated and developed the Cartesian impedance controller (presented in Chapter~\ref{chp:hsacontrol}) and constructed the experimental setup (e.g., motion capture system, setpoint projection, mounting the hairspray container). S.S.B. implemented and trained the \gls{EEG} data processing and classification pipeline.
C.D.S. and S.C. supervised the project and provided funding.
}


%% Start the actual chapter on a new page.
\newpage

\input{braincontrol/sections/S01_introduction}
\input{braincontrol/sections/S02_methodology}
\input{braincontrol/sections/S03_experiments}
\input{braincontrol/sections/S04_results}
\input{braincontrol/sections/S05_conclusion}

\section*{Afterword}
% In this chapter, we introduced a \gls{BMI} protocol for soft robots based on motor imagery with wearable, few-channel \gls{EEG} devices. Even though the binary motor imagery classifier is only correct in \~\SI{70}{\percent} of cases, we demonstrated experimentally effective assistance with an activity of daily living and safe operation. However, operating the soft robot with motor imagery takes the user's full concentration and focus and can be very exhausting when done for more than a few minutes. Therefore, we would like to aim for fully autonomous (soft) robots, and instead of communicating low-level end-effector setpoints to the robot, we prefer to communicate high-level task assignments. In Chapter~\ref{chp:osmp}, we take a step towards this goal by developing a strategy for learning compliant and stable motion policies for period movements from demonstration. This allows the user to demonstrate the (periodic) movement to accomplish a given task once, and then, when assigned this task, the (soft) robot can execute this task in a compliant, stable, and safe fashion.
In this chapter, we introduced a \gls{BMI} protocol for soft robots using motor imagery with wearable, few-channel \gls{EEG} devices. However, controlling the soft robot via motor imagery demands the user’s full attention and can become highly exhausting after a few minutes. Consequently, our goal is to develop fully autonomous (soft) robots. Rather than conveying low-level end-effector setpoints, we aim to communicate high-level task assignments. In Chapter~\ref{chp:osmp}, we take a first step toward this objective by presenting a strategy for learning compliant and stable motion policies for periodic movements from demonstration. This approach enables the user to demonstrate the required (periodic) movement once, allowing the (soft) robot to perform the task autonomously in a compliant, stable, and safe manner.
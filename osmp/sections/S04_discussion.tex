\section{Discussion}\label{sec:osmp:discussion}
\paragraph{Convergence Guarantees}
The first result introduced in the previous section is theoretical in nature: we establish that the proposed \gls{OSMP} guarantees almost-global asymptotic orbital stability. Under mild architectural conditions, this further strengthens into almost-global exponential orbital stability via transverse contraction. Yet this result is not merely of theoretical interest—it carries profound practical implications. Indeed, it ensures that, regardless of network weights or conditioning, any trajectory converges exponentially fast to the learned limit cycle. Exponential convergence prevents slow-dynamics plateaus where the system might otherwise stall, while the near-global basin drastically reduces the need for far-reaching training data coverage. Together, these properties yield significant gains in data efficiency, as they obviate the need to densely sample the state space. Beyond training, exponential convergence also facilitates system-level analysis and enhances modularity. Most critically, contractiveness plays a central role in enabling policy reuse and transfer learning: while interconnecting merely stable systems can lead to instability, contracting systems are provably composable and lend themselves naturally to transfer and hierarchical control~\citep{ofir2022sufficient,angeli2025lmi}. 

In the same spirit, it is important to underscore that our stability results hold uniformly for any fixed conditioning value $z$, thereby reinforcing their relevance for transferability. This property suggests robustness not only to different instances within a task family but also to transitions across them. While outside the current scope, it is natural to ask whether convergence still holds under time-varying conditioning $z(t)$, such as during policy switching. Thanks to the global nature of the underlying result, the extension to the case where $z(t)$ eventually stabilizes (i.e., $z(t) \equiv z_\infty$ for $t > t'$) is straightforward. More ambitiously, we believe that the exponential convergence rate can be leveraged to establish stability even under piecewise-constant $z(t)$, including non-smooth transitions over finite horizons. This opens the door to formal tools for designing learning-based schedules of optimal conditioning patterns—a direction we leave for future investigation.

\paragraph{Quantitative Benchmark}
Quantitative benchmarks show that \glspl{OSMP} surpass most baselines across the majority of dataset categories. They display superior convergence — especially when initialized far from the demonstration—compared with classic neural motion policies such as \glspl{MLP}, \glspl{RNN}, \glspl{LSTM}, \glspl{NODE}, and even state-of-the-art \glspl{DP}~\citep{chi2023diffusion}. While other orbitally stable approaches like iFlow~\citep{urain2020imitationflow} and \gls{SPDT}~\citep{zhi2024teaching} guarantee convergence to a limit cycle, they often struggle to (a) imitate highly curved shapes or discontinuous velocity profiles and (b) ensure that the resulting limit cycle accurately reproduces those complex demonstrations.
For example, compared to SPDT~\citep{zhi2024teaching}, \glspl{OSMP} are able to imitate the velocity profile, and their limit cycle captures the oracle shape 5x more accurately, as the global convergence analysis shows.
The few cases in which baselines outperform \glspl{OSMP} highlight avenues for improvement. On the IROS letters dataset~\citep{urain2020imitationflow}, DPs~\citep{chi2023diffusion} achieve lower imitation errors, likely because widely separated demonstrations of the same letter must be captured by a single policy, which favors probabilistic methods like \glspl{DP} and iFlow over deterministic ones, such as NODEs, SPDT, and \glspl{OSMP}. For image-contour data, MLP policies slightly edge out \glspl{OSMP} when starting near the demonstration but degrade markedly when initialized farther away.
Regarding inference time/runtime, \glspl{OSMP} demand more computation than classical neural policies (which forgo gradient evaluations during inference) and iFlow~\citep{urain2020imitationflow} (given our more expressive bijective encoders), yet they strike a favorable performance-to-inference-time balance and are 10–20× faster than standard diffusion policies~\citep{chi2023diffusion}. Note, too, that the timings in Table~\ref{tab:osmp:benchmarking_quantitative_results} stem from unoptimized, eager-mode inference; with numerical gradients and PyTorch ahead-of-time compilation, \glspl{OSMP} can run at up to \SI{15}{kHz} on modern CPUs, as shown in Tab.~\ref{tab:osmp:inference_time_benchmarking}.

\paragraph{Real-World Experiments}
When moving to real-world experiments, we quickly realized how the interpretable latent-dynamics structure allows us to inspect post-training how closely the learned cycle mirrors the periodic demonstration, promoting predictable behavior when the policy is deployed on a real robot.
This aspect will require further quantification in future work.

On the UR5 robot, \glspl{OSMP} are able to track the oracle with directed Hausdorff distance between \SI{3}{mm} and \SI{14}{mm}. As the Helix soft robot employs a less accurate low level controller, the shape tracking accuracy drops to $6-11$~\si{mm}. Still, \glspl{OSMP} outperform classical trajectory tracking controller in terms of shape accuracy on the Helix soft robot by approximately $60-65$~\%.

\glspl{OSMP} enable highly successful locomotion in the turtle robot by leveraging biologically derived swimming oracles. Although oracle tracking is not perfectly precise (directed Hausdorff distance of $0.085$-$0.382$~rad)—especially at higher speeds due to motor limits and external disturbances like water drag—the system remains stable, quickly reconverges to the limit cycle, and keeps the limbs nearly perfectly synchronized.

Next, we trained individual \glspl{OSMP} on multiple kinesthetic-teaching demonstrations for the UR5 and KUKA manipulators. Although some demonstrations were jerky and uneven, the \glspl{OSMP} accurately captured the intended motions: the UR5 achieved a \SI{100}{\percent} success rate in cleaning the whiteboard using the learned policy. Similarly, the KUKA robot consistently completed the brooming task over many repetitions—despite limited execution speed and tracking accuracy imposed by the low feedback gains of its impedance controller—and even succeeded when its motion was perturbed by external disturbances.

Although orbital stability is always guaranteed, large deformations in the learned diffeomorphism can push the system far from the demonstrated path. On real robots, that drift is risky because joint position and velocity limits—or a finite task-space workspace—can be violated. We saw this on the Crush turtle robot, whose joint range and velocity/acceleration caps constrain what the low-level controller can track. Two design choices proved helpful: (i) encoder regularization during training and (ii) a sliding-mode–style motion modulation that first draws the system into a neighborhood of the oracle before advancing along the polar phase. Since motion directly on the oracle is usually feasible, these measures prevent most of the problems. Looking ahead, embedding \glspl{CBF} could explicitly keep the system out of infeasible or unsafe regions in the oracle space, echoing recent advances in the \gls{DMP} literature~\citep{davoodi2022rule,nawaz2024learning,mohammadi2024extended,simmoteit2025diffeomorphic}.

Phase synchronization is vital when deploying \glspl{OSMP} for turtle swimming: without it, the limb controllers—lacking an explicit time parameter—would drift out of phase, sharply increasing the cost of transport. Instead, our phase synchronization strategy is able to keep the mean phase error between the two flippers at less than \SI{0.2}{\degree}.
The same synchronization strategy can be applied in the future to other platforms, such as bipedal or quadrupedal robots. We also demonstrate that training separate \glspl{OSMP} and synchronizing their phases at runtime can outperform a single large \gls{OSMP}, as disturbances affecting one subsystem do not pull the others away from their limit cycles.

Conditioning \glspl{DMP} on task information enables them to produce purposeful motions for tasks never encountered during training, marking a paradigm shift that opens the door to zero-shot transfer of convergence-guaranteed motion policies to far more complex, unseen scenarios. Here, we take an initial step toward that vision: our training procedure enables smooth interpolation between motions observed in the dataset. For instance, a single \gls{OSMP} trained on both forward and reverse turtle swimming can effortlessly blend the two behaviors into a continuum.
Crucially, the orbital stability is always preserved, even when the shape of the stable limit cycle changes as a function of the conditioning $z$.


\paragraph{Limitations}
The proposed \gls{OSMP} exhibits several limitations that highlight promising directions for future development. First, it presumes prior annotation of the demonstration’s attractor regime, requiring that each trajectory be segmented to isolate the periodic portion and implicitly modeled as a limit cycle. While our approach accommodates sharp turns and discontinuities more robustly than existing baselines, such features still induce sizable deformations in the learned vector field. These deformations manifest as locally aggressive dynamics, where nearby trajectories may temporarily diverge markedly before reconverging toward the limit cycle.

Second, like other time-invariant dynamical motion primitive frameworks~\citep{ijspeert2002learning, ijspeert2013dynamical, rana2020euclideanizing, perez2023stable}, \gls{OSMP} inherits the limitation of being ill-posed under intersecting demonstrations or oracles—situations that demand multi-valued flows at a single point in state space. In addition to existing ideas in literature~\citep{sun2024directionality}, formulating the motion policy as second-order dynamics or augmenting the dynamics with an explicit notion of trajectory progress could resolve this ambiguity by lifting the state into a higher-dimensional phase space.

Looking ahead, we envision extending the framework to support multiple classes of attractors—such as point attractors with \gls{GAS}, multistable basins with \gls{MS}, and limit cycles with \gls{OS}—within a single primitive. The supercritical Hopf bifurcation already captures a transition between equilibrium and periodic behavior, suggesting that the inclusion of an additional “attractor type” parameter could generalize the formalism to support richer behaviors~\citep{strogatz2018nonlinear}. Finally, replacing the explicit conditioning variable $z$ with implicit observation-based embeddings (e.g., task images or object states) would make \gls{OSMP} compatible with vision-language-action models such as $\pi\_0$~\citep{black2024pi0} and SmolVLA~\citep{shukor2025smolvla}, offering a path toward more expressive and versatile control policies.
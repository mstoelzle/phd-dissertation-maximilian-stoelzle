\section{Methodology}

In this work, we introduce \glspl{OSMP}, which can be trained to capture complex periodic motions from demonstrations while ensuring convergence to a limit cycle that aligns with a predefined oracle. To accomplish this, we build on previous research~\citep{rana2020euclideanizing, zhi2024teaching} that combines learned bijective encoders with a prescribed motion behavior in latent space. These latent dynamics generate velocities or accelerations that are subsequently mapped back into the oracle space via a pullback operator~\citep{zhang2022learning}—in the case of a bijective encoder, this operator is the inverse of the encoder’s Jacobian. In this formulation, the motion in latent space exhibits key convergence properties, such as Global Asymptotic Stability (GAS)~\citep{rana2020euclideanizing, perez2023stable, sochopoulos2024learning} or Orbital Stability (OS)~\citep{zhi2024teaching}, while the learned encoder provides the necessary expressiveness to capture complex motions and transfers the convergence guarantees from latent to oracle space through the established diffeomorphism.

However, compared to existing work~\citep{zhi2024teaching}, we introduce several crucial modifications that enhance both the performance and practical utility of the proposed method: (1) we develop a limit cycle matching loss to reduce the discrepancy between the learned limit cycle and the periodic oracle; (2) we design strategies to modulate the learned velocity field online without the need for retraining—for instance, to adjust the convergence behavior; (3) we establish a procedure to synchronize the phase of multiple \glspl{OSMP}; and (4) we condition the encoder on a task, enabling a single \gls{OSMP} to exhibit multiple distinct motion behaviors. Moreover, we introduce loss terms that allow the trained \gls{OSMP} to smoothly interpolate between the learned motion behaviors—something that has not been possible before.

\subsection{Orbitally Stable Motion Primitives}
The dynamical motion policy $\dot{x}=f(x; z)$ is typically defined in task space but can also be defined in other Cartesian or generalized coordinates (e.g., joint space). Therefore, we will in the following refer to these coordinates as \emph{Oracle space}.
In this work, we are specifically interested in cases where we train $f(x)$ to learn periodic motions.
Then, the smooth diffeomorphism between the oracle and latent space is made via a bijective encoder $\Psi: \mathbb{R}^n \to \mathbb{R}^n$, which maps positional states $x \in \mathbb{R}^n$ into the latent states $y \in \mathbb{R}^n$.
Optionally, this encoding is conditioned on a continuous variable $z \in \mathbb{R}$ as a homotopy such that $y = \Psi(x;z)$.
Furthermore, we construct $\Psi$ such that it is invertible and a closed-form inverse function $\Psi^{-1}: y \mapsto x$ allows us to map from latent space back into oracle space.
In latent space, we apply dynamics $\dot{y} = f_\mathrm{y}(y)$ that exhibit a stable limit cycle behavior. In summary, the orbitally stable motion primitive is given as
\begin{equation}\label{eq:dynamics}
    \dot{x} = f(x;z) = f_\mathrm{s}(x) \, J_\Psi^{-1}(x;z) \, f_\mathrm{y} \left (\Psi(x;z) \right ),
\end{equation}\
where $J_\Psi = \frac{\partial \Psi(x;z)}{\partial x}$ defines the Jacobian of the encoder. 
The function $f_\mathrm{s}(x): \mathbb{R}^n \to \mathbb{R}_{>0}$ scales the velocity magnitude and is implemented as $f_\mathrm{s}(x) = e^{\mathrm{MLP}(x)} + \varepsilon$, where $\varepsilon \in \mathbb{R}_{>0}$ is a small value~\citep{rana2020euclideanizing}.
As $\Psi$ is a diffeomorphism (w.r.t. $x$ and $y$), the motion policy is orbitally stable by construction~\citep{rana2020euclideanizing, zhi2024teaching}.

\subsubsection{Diffeomorphic Encoder}
We consider a bijective encoder $\Psi_\theta : \mathbb{R}^n \times \mathbb{R} \to \mathbb{R}^n$, which maps positional states $x \in \mathbb{R}^n$ into the latent states $y \in \mathbb{R}^n$ conditioned on $z \in \mathbb{R}$, where we assume that $n \in \mathbb{N} \geq 2$.
The encoder $y = \Psi_\theta(x;z)$ adopting the Euclideanizing flows~\citep{dinh2017density, rana2020euclideanizing} architecture is parametrized by the learnable weights $\theta \in \mathbb{R}^{n_\theta}$ and consists of $n_\mathrm{b}$ blocks, where each block is analytically invertible.
If a conditioning is used, $z$ is first lifted into an embedding $\Bar{z}$, which is subsequently used to condition each block.
More details on the encoder architecture can be found in the pioneering work~\citep{dinh2017density, rana2020euclideanizing} and Section~\ref{sub:osmp:methodology:training}.

\subsubsection{Latent Dynamics}
In latent space, we consider the 1st-order dynamics of a supercritical Hopf bifurcation~\citep{strogatz2018nonlinear, khadivar2021learning, zhi2024teaching, nah2025combining}
\begin{equation}\label{eq:osmp:latent_dynamics}
    \dot{y} = \begin{bmatrix}
        \dot{y}_1\\
        \dot{y}_2\\
        \dot{y}_{3:n}\\
    \end{bmatrix} = f_\mathrm{y}(y) = \begin{bmatrix}
        -\omega(y) \, y_2 + \alpha \, \left ( 1 - \frac{y_1^2 + y_2^2}{R^2} \right ) \, y_1\\
        + \omega(y) \, y_1 + \alpha \, \left ( 1 - \frac{y_1^2 + y_2^2}{R^2} \right ) \, y_2\\
        -\beta \, y_{3:n}\\
    \end{bmatrix},
    % \quad
    % \forall i \: \in 3, \dots, N.
\end{equation}
where $\omega(y) = f_\omega(\operatorname{atan2}(y_2, y_1)) > 0$ determines the angular velocity.
Here, the dynamics of $y_1$ and $y_2$ describe the Cartesian-space behavior of a simple limit cycle whose behavior in polar coordinates $y_\mathrm{pol} = \begin{bmatrix}
    r & \varphi & y_{3:n}^\top
\end{bmatrix}^\top$ is expressed as
\begin{equation}\label{eq:osmp:latent_dynamics_polar_coordinates}
    \dot{y}_\mathrm{pol} = \begin{bmatrix}
        \dot{r}\\ \dot{\varphi}\\ \dot{y}_{3:n}
    \end{bmatrix} = f_\mathrm{pol}(y_\mathrm{pol}) = \begin{bmatrix}
        \alpha \left ( 1 - \frac{r^2}{R^2} \right ) \, r\\ f_\omega(\varphi)\\ -\beta \, y_{3:n}\\
    \end{bmatrix},
    % \quad
    % \forall i \: \in 3, \dots, N,
\end{equation}
where $f_\omega(\varphi): [-\pi, \pi) \to \mathbb{R}_{>0}$ computes the positive angular velocity as a function of the polar angle. Often, in particular, when employing $f_\mathrm{s}(x) \neq 1$, it can also be set to a constant: $\omega = 1$. 
If not, we define in practice $\omega(y) = f_\omega(\Bar{y}_{1:2}) = e^{\mathrm{MLP}(\Bar{y}_{1:2})} + \epsilon_\omega$, where $\Bar{y}_{1:2} = \begin{bmatrix}
    \frac{y_1}{\sqrt{y_1^2+y_2^2}} &  \frac{y_2}{\sqrt{y_1^2+y_2^2}}
\end{bmatrix}^\top \in \mathbb{R}^2$ with $\epsilon_\omega > 0$.

$\alpha > 0$ and $\beta > 0$ are positive gains that determine how fast the system converges onto the limit cycle.  When learning the dynamics, we choose $\alpha = \beta = 1$. $R \in \mathbb{R}_{>0}$ expresses the radius of the limit cycle in latent space. Again, it is sufficient to choose $R =1$ or $R=0.5$.
% Finally, latent space velocity is mapped back into the original space using the inverse Jacobian of the encoder $\dot{x} = J_\Psi^{-1} \, \dot{y}$.

\paragraph{Map from Polar to Cartesian Coordinates}
The map $h_{\mathrm{p2y}}(y_\mathrm{pol}): \mathbb{R}^n \to \mathbb{R}^n$ from latent polar coordinates to latent Cartesian coordinates and its inverse $h_{\mathrm{y2p}}(y) = h_{\mathrm{p2y}}^{-1}(y)$ is given by
\begin{equation}
    y = h_{\mathrm{p2c}}(y_\mathrm{pol}) = \begin{bmatrix}
        r \, \cos(\varphi)\\
        r \, \sin(\varphi)\\
        y_{3:n}
    \end{bmatrix},
    \qquad
    y_\mathrm{pol} = h_{\mathrm{c2p}}(y) = \begin{bmatrix}
        \sqrt{y_1^2+y_2^2}\\
        \operatorname{atan2}(y_2, y_1)\\
        y_{3:n}
    \end{bmatrix}.
\end{equation}
Then, the Jacobian of the Polar-to-Cartesian map $\frac{\partial h_{\mathrm{p2c}}}{\partial y_\mathrm{pol}} \in \mathbb{R}^{n \times n}$ is given by
\begin{equation}
    \frac{\partial h_{\mathrm{p2c}}}{\partial y_\mathrm{pol}} = \begin{bmatrix}
        \cos(\varphi) & -r \, \sin(\varphi) & 0_{1 \times (n-2)}\\
        \sin(\varphi) & r \, \cos(\varphi) & 0_{1 \times (n-2)}\\
        0_{(n-2) \times 1} & 0_{(n-2) \times 1} & \mathbb{I}_{n-2}
    \end{bmatrix}
\end{equation}
We can also substitute $y_\mathrm{pol} = y$ into the Jacobian and determine its inverse
\begin{equation}
    \frac{\partial h_{\mathrm{p2c}}}{\partial y_\mathrm{pol}} = \begin{bmatrix}
        \frac{y_1}{\sqrt{y_1^2 + y_2^2}} & -y_2 & 0_{1 \times (n-2)}\\
        \frac{y_2}{\sqrt{y_1^2 + y_2^2}} & y_1 & 0_{1 \times (n-2)}\\
        0_{(n-2) \times 1} & 0_{(n-2) \times 1} & \mathbb{I}_{n-2}
    \end{bmatrix},
    \qquad
    \frac{\partial h_{\mathrm{p2c}}}{\partial y_\mathrm{pol}}^{-1} = \begin{bmatrix}
        \frac{y_1}{\sqrt{y_1^2 + y_2^2}} & \frac{y_2}{\sqrt{y_1^2 + y_2^2}} & 0_{1 \times (n-2)}\\
        -\frac{y_2}{y_1^2 + y_2^2} & \frac{y_1}{y_1^2 + y_2^2} & 0_{1 \times (n-2)}\\
        0_{(n-2) \times 1} & 0_{(n-2) \times 1} & \mathbb{I}_{n-2}
    \end{bmatrix},
\end{equation}
which are both full-rank for $r = \sqrt{y_1^2 + y_2^2} > 0$.

\subsection{Training}\label{sub:osmp:methodology:training}
We consider a dataset $\langle T, X^\mathrm{d}, \dot{X}^\mathrm{d}, Z \rangle$ as a tuple between timestamps $T = \langle t(1), \dots, t(k), \dots, t(N) \rangle$ positions $X^\mathrm{d} = \langle x^\mathrm{d}(1), \dots, x^\mathrm{d}(k), \dots, x^\mathrm{d}(\mathrm{N}) \rangle$, the corresponding, demonstrated velocities $\dot{X}^\mathrm{d} = \langle \dot{x}^\mathrm{d}(1), \dots, \dot{x}^\mathrm{d}(k), \dots, \dot{x}^\mathrm{d}(\mathrm{N}) \rangle$ and an optional conditioning $Z = \langle z(1), \dots, z(k), \dots, z(\mathrm{N}) \rangle$, where $k \in \mathbb{N}_\mathrm{N} = \{1, 2, \dots, N \}$.
%

\subsubsection{Bijective Encoder}
The bijective encoder based on Euclideanizing flow~\citep{rana2020euclideanizing} / Real NVP~\citep{dinh2017density} uses coupling layers with the scaling and translation functions parametrized by \glspl{RFFN} that integrate ELU activation functions and a hidden dimension of $100$.
The number of encoder layers/blocks varies by the complexity of the task and ranges from $10$ for simple tasks to $25$ for very complex tasks.
At the start of the training, the encoder is initialized as an identity mapping.

\subsubsection{(Angular) Velocity Scaling Network}
The optional velocity scaling network $e^{\mathrm{MLP}(x)}$ is parametrized by a three-to-five-layer MLP (depending on the nonlinearities and discontinuities that the demonstration velocity profile exhibits) with a hidden dimension of $128$ and LeakyReLU activation functions.

Similarly, the angular velocity network $\omega(y) = f_\omega(\Bar{y}_{1:2}) = e^{\mathrm{MLP}(\Bar{y}_{1:2})} + \epsilon_\omega$, where $\Bar{y}_{1:2} = \begin{bmatrix}
    \frac{y_1}{\sqrt{y_1^2+y_2^2}} &  \frac{y_2}{\sqrt{y_1^2+y_2^2}}
\end{bmatrix}^\top \in \mathbb{R}^2$ with $\epsilon_\omega = 10^{-6}$, is parametrized by a five-layer MLP with a hidden dimension of $128$ and LeakyReLU activation functions.

\subsubsection{Loss Functions}
The total training loss function is then given by
\begin{equation}
    \mathcal{L} = \underbrace{\zeta_\mathrm{vi} \, \mathcal{L}_\mathrm{vi}}_\text{Vel. Imitation} + \underbrace{\zeta_\mathrm{lcm} \, \mathcal{L}_\mathrm{lcm}}_\text{Limit Cycle Matching} + \underbrace{\zeta_\mathrm{tgd} \, \mathcal{L}_\mathrm{tgd}}_\text{Time Guidance} 
    + \underbrace{\zeta_\mathrm{er} \, \mathcal{L}_{\mathrm{er}}}_\text{Encoder Reg.}
    + \underbrace{\zeta_\mathrm{vr} \, \mathcal{L}_{\mathrm{vr}}}_\text{Vel. Reg.}  + \underbrace{\zeta_\mathrm{sci} \, \mathcal{L}_{\mathrm{sci}}}_\text{Cond. Interpolation} 
    % + \underbrace{\zeta_\mathrm{wr} \, \mathcal{L}_\mathrm{wr}}_\text{NN Weight Reg.},
\end{equation}
where $\zeta_\mathrm{vi}, \zeta_\mathrm{lcm}, \zeta_\mathrm{tgd}, \zeta_\mathrm{er}, \zeta_\mathrm{vr}, \zeta_\mathrm{sci} \in \mathbb{R}$ are scalar loss weights.
$\mathcal{L}_\mathrm{vi}$ is a loss term that enforces that the velocity of the motion primitive matches the one demanded by the demonstration at all samples in the demonstration dataset, $\mathcal{L}_\mathrm{lcm}$ ensures that the encoded demonstration positions lie on the latent limit cycle. The time-guidance loss $\mathcal{L}_\mathrm{tgd}$ can support the limit cycle matching loss for highly curved demonstrations. 
The term $\mathcal{L}_\mathrm{er}$ regularizes the encoder by penalizing deviations from the identity encoder.
The optional $\mathcal{L}_{\mathrm{sci}}$ gives rise to smooth interpolation between different encoder conditioning.
% , and $\mathcal{L}_\mathrm{wr}$ regularizes the encoder network weights.
The discretionary velocity regularization loss $\zeta_\mathrm{vr}$ increases the numerical stability by penalizing very high velocities. In the following, we define each loss term formally.

\paragraph{Velocity Imitation Loss}
Analog to the literature on stable point-to-point motion primitives~\citep{rana2020euclideanizing}, the predicted oracle space velocity is supervised by a smooth $\ell_1$ loss~\citep{girshick2015fast}, which computes a squared term if the absolute error falls below $\beta_{\ell_1}$ and the $\ell_1$ term otherwise, and can be formally defined as
\begin{equation}
    % \mathcal{L}_\mathrm{vi} = \sum_{k = 1}^{N} \frac{\lVert \dot{x}^\mathrm{d}(k) - f(x^\mathrm{d}(k);z(k)) \rVert_2^2}{N}.
    \mathcal{L}_\mathrm{vi} = \frac{1}{N} \, \sum_{k = 1}^{N} \begin{cases}
		\frac{\left ( f(x^\mathrm{d}(k);z(k)) - \dot{x}^\mathrm{d}(k) \right )^2}{2 \, \ell_1}, & \text{if} \: \left \lVert f(x^\mathrm{d}(k);z(k)) - \dot{x}^\mathrm{d}(k) \right \rVert_1 < \beta_{\ell_1} \\
        \left \lVert f(x^\mathrm{d}(k);z(k)) - \dot{x}^\mathrm{d}(k) \right \rVert_1 - \frac{\beta_{\ell_1}}{2} , & \text{otherwise}
    \end{cases},
\end{equation}
where we choose $\beta_{\ell_1} = 1$.

\paragraph{Limit Cycle Matching Loss}
Next, we consider a subset of the demonstrations $\mathcal{P} \subset \mathbb{N}_\mathrm{N}$ that exhibit a periodic motion. To guarantee the OS of the learned system, we need to make sure that the learned limit cycle matches the periodic demonstration.
For this purpose, we design a \emph{limit cycle matching} loss $\mathcal{L}_\mathrm{lcm}$ in latent space
% \begin{equation}
% \begin{split}
%     y_\mathrm{p}(k) =& \: \begin{bmatrix}
%         \sqrt{y_1^2(k) + y_2^2(k)} & y_3 & \dots & y_n
%     \end{bmatrix}^\mathrm{T}, \\
%     y_\mathrm{p}^\mathrm{d}(k) =& \: \begin{bmatrix}
%         R & 0 & \dots & 0
%     \end{bmatrix}^\mathrm{T}, \\
%     \mathcal{L}_\mathrm{lcm} =& \: \sum_{k \in \mathcal{P}} \frac{\big \lVert y_\mathrm{p}^\mathrm{d}(k) - y_\mathrm{p}(k) \big \rVert_2^2}{|\mathcal{P}|},
% \end{split}
% \end{equation}
\begin{equation}\small
    y_\mathrm{p}(k) = \begin{bmatrix}
        \sqrt{y_1^2(k) + y_2^2(k)}\\ y_{3:n}
    \end{bmatrix} \in \mathbb{R}^{n-1},
    \quad
    y_\mathrm{p}^\mathrm{d}(k) = \begin{bmatrix}
        R\\ 0_{n-2}
    \end{bmatrix},
    \quad
    \mathcal{L}_\mathrm{lcm} = \sum_{k \in \mathcal{P}} \frac{\big \lVert y_\mathrm{p}^\mathrm{d}(k) - y_\mathrm{p}(k) \big \rVert_2^2}{|\mathcal{P}|},
\end{equation}
where $|\mathcal{P}|$ is the cardinality of $\mathcal{P}$, and $y=\Psi(x^\mathrm{d}; z) \in \mathbb{R}^n$ is the latent encoding. % , and $y_\mathrm{p}^\mathrm{d}$ and $y_\mathrm{p}$ are the desired and actual latent positions in polar coordinates, respectively.

\paragraph{Time Reference Guidance Loss}
% For highly curved oracles, we found it to be helpful to guide the mapping of the oracle positions onto latent polar angle by leveraging the time parametrization of the oracle. The idea is to evenly distribute the oracle positions onto the latent-space limit cycle and preventing the encoders to be clustered in one region of the latent limit cycle while other polar angular regions do not have a correspondence on the oracle.
For strongly curved oracles, we have found it advantageous to use the oracle’s time parameterization to steer how its positions map onto the latent polar angle. Doing so spreads the oracle samples uniformly around the latent-space limit cycle, preventing the encoders from bunching up in one angular sector while leaving other portions of the cycle without corresponding oracle points.

First, we compute for each position contained in the rhythmic demonstration a desired latent polar angle based on the normalized time reference. Simultaneously, we evaluate the actual encoded polar angle of $x^\mathrm{d}(k)$ as
\begin{equation}
     \varphi^\mathrm{d}(k) = \varphi_0 + 2 \, \pi \, \frac{t(k)}{P},
     \qquad
     \varphi(k) = \operatorname{atan2}\left ( \Psi(x(k);z(k))_2, \{ \Psi(x(k);z(k)) \}_1 \right),
\end{equation}
where $\varphi_0$ is the polar angle anchor and $P$ is the period of the rhythmic demonstration.
Subsequently, we define a positive alignment loss between the 
\begin{equation}
    \mathcal{L}_\mathrm{tgd} = \sum_{k \in \mathcal{P}} \frac{\max \left ( \left | \operatorname{mod} \bigl( \varphi^\mathrm{d}(k) - \varphi(k) + \pi,\; 2\pi\bigr) - \pi \right | - m_\mathrm{tgd}, 0 \right )^2}{|\mathcal{P}|},
\end{equation}
$m_\mathrm{tgd} \in \mathbb{R}_{>0}$ is the allowed margin between the time reference and the actual polar phase and the function $n_{e_\varphi}(e_\varphi) = \operatorname{mod} \bigl( e_\varphi(k) + \pi,\; 2\pi\bigr) - \pi$ normalizes the polar angle error $e_\varphi(k) = \varphi^\mathrm{d}(k) - \varphi(k) $ into the interval $[-\pi, \pi)$.

\paragraph{Encoder Regularization}
Similar to Zhi \textit{et al.}~\citep{zhi2024teaching}, we employ an encoder regularization loss $\mathcal{L}_{\Psi, \mathrm{reg}}$ that penalizes deviations from an identity encoder $y = \Psi(x) = x$.
We draw in each epoch $N$ random positions samples $x(k) ~\sim \mathcal{U}(x_\mathrm{min}, x_\mathrm{max}) \: \forall \, k \in \mathbb{N}_N$ from a uniform distribution within the workspace $[x_\mathrm{min}, x_\mathrm{max}]$ of the system. Then, the loss is computed as
\begin{equation}
    \mathcal{L}_{\mathrm{er}} = \sum_{k=1}^{N} \frac{\lVert x - \Psi(x;z) \rVert}{N}.
\end{equation}


\paragraph{Velocity Regularization}
% The velocity imitation loss $\mathcal{L}_\mathrm{vi}$ only supervises the velocity on the demonstration. However, in particular in cases with very few demonstrations that are close to each other, the velocity magnitude remains unsupervised in many parts of the system workspace, even though orbital stability and transverse contraction always remains ensured. However, in practice, large predicted velocities frequently lead to numerical instability. Therefore, it can be helpful to regularize the predicted velocities.
The velocity-imitation loss $\mathcal{L}_\mathrm{vi}$ constrains the predicted velocities only along the demonstration trajectory. When demonstrations are sparse and clustered, large regions of the workspace receive no direct supervision on velocity magnitude, even though orbital stability and transverse contraction are still guaranteed. However, in practice, large predicted velocities frequently lead to numerical instability. Therefore, it can be helpful to regularize the predicted velocities.

For this purpose, we draw in each epoch $N$ random positions samples 
\begin{equation}
    x(k) ~\sim \mathcal{U}(x_\mathrm{min}, x_\mathrm{max}) \: \forall \, k \in \mathbb{N}_N
\end{equation}
from a uniform distribution within the workspace $[x_\mathrm{min}, x_\mathrm{max}]$ of the system. Then, the loss is computed as
\begin{equation}
    \mathcal{L}_\mathrm{vr} = \sum_{k=1}^N \frac{\max(\lVert f(x(k);z)\rVert_2 - m_\mathrm{vr}, 0_N)}{N},
\end{equation}
where $m \in \mathbb{R}_{\geq0}$ is the margin. In practice, we choose $m_\mathrm{vr}$ to be 50\% higher than the maximum velocity magnitude in the dataset in order not to conflict with the $\mathcal{L}_\mathrm{vi}$ objective.


\paragraph{Smooth Conditioning Interpolation Loss}
Next, optionally, we can add a loss term that encourages a smooth interpolation of the learned limit cycle between conditionings $z$. We assume that all conditionings in the dataset $z(k) \in \mathcal{Z}$, where $\mathcal{Z} = \{ z(1), \dots, z(k), \dots, z(N) \}$, are bounded in the interval $[z_\mathrm{min}, z_\mathrm{max}]$.
Now, consider a convex hull $\operatorname{conv}(\mathcal{Z}) = [\min(\mathcal{Z}), \min(\mathcal{Z})] = [z_\mathrm{min}, z_\mathrm{max}]$.
Next, we draw $N_\mathrm{sci}$ random conditionings from a uniform distribution: $\tilde{z}(j) \sim \mathcal{U}(\operatorname{conv}(\mathcal{Z})) \in \mathbb{R}$ with $j \in \mathbb{N}_{N_\mathrm{sci}}$.
Additionally, we also generate $N_\mathrm{sci}$ samples on the latent limit cycle by uniformly sampling polar angles $\varphi(j) \sim [-\pi, \pi)$ and subsequently first map into Cartesian latent coordinates and then into oracle space using the inverse encoder
\begin{equation}
    y(j) = \begin{bmatrix}
        R \, \cos(\varphi(j)) & R \, \sin(\varphi(j)) & 0_{n-2}^\top
    \end{bmatrix}^\mathrm{T},
    \quad
    \tilde{x}(j) = \Psi^{-1}(y(j) \, ; \tilde{z}(j)).
\end{equation}
Now, we define the floor $\lfloor \cdot \rfloor$ and ceil $\lceil \cdot \rceil$ functions that round down, or up to the next conditioning $z \in \mathcal{Z}$ in the dataset
\begin{equation}
    \lfloor \tilde{z} \rfloor = \max\{ z \in \mathbb{Z} \mid z \le \tilde{z} \},
    \qquad
    \lceil \tilde{z} \rceil = \min\{ z \in \mathbb{Z} \mid z \ge \tilde{z} \}.
\end{equation}
Then, the target for $\tilde{x}(j)$ that represents a smooth linear interpolation between conditioning $\lfloor \tilde{z} \rfloor $ and $\lceil \tilde{z} \rceil$ is given by
\begin{equation}
    \tilde{x}^*(j) = \lfloor \tilde{x}(j) \rfloor + \frac{\tilde{z}(j) - \lfloor \tilde{z}(j) \rfloor}{\lceil \tilde{z}(j) \rceil - \lfloor \tilde{z}(j) \rfloor} \left ( \lceil \tilde{x}(j) \rceil - \lfloor \tilde{x}(j) \rfloor \right )
\end{equation}
where
\begin{equation}
    \lfloor \tilde{x}(j) \rfloor = \Psi^{-1}(y(j) \, ; \lfloor \tilde{z}(j) \rfloor),
    \qquad
    \lceil \tilde{x}(j) \rceil = \Psi^{-1}(y(j) \, ; \lceil \tilde{z}(j) \rceil).
\end{equation}
Finally, the smooth conditioning interpolation loss can be formulated as
\begin{equation}
    \mathcal{L}_{\mathrm{sci}} = \sum_{j = 1}^{N_\mathrm{sci}} \frac{\left ( \tilde{x}^*(j) - \tilde{x}(j)\right )^2}{N_\mathrm{sci}}.
\end{equation}

% Corresponding to each loss term, $\zeta_\mathrm{vi}, \zeta_\mathrm{lcm}, \zeta_\mathrm{tgd}, \zeta_\mathrm{er}, \zeta_\mathrm{vr}, \zeta_\mathrm{sci} \in \mathbb{R}$ are scalar loss weights.
% Moreover, we penalize the deformation of the diffeomorphism by regularizing the weights $\theta$ of the bijective encoder: $\mathcal{L}_\mathrm{wr} = \sum_{w=1}^{n_\theta} \frac{\theta_w}{n_\theta}$.

\subsection{Inference}
Maintaining discrete-time stability demands that the \gls{OSMP}—or any \gls{DMP}—runs at sufficiently high control rates. This requirement becomes even tougher when computational resources are limited, as in our turtle-swimming setup where the \glspl{OSMP} ran on a Raspberry Pi 5. To minimise latency, we sought to shorten the \gls{OSMP}’s inference time by exploiting PyTorch’s compilation and export utilities. Unfortunately, most current PyTorch compilers/exporters are incompatible with autograd, which we still need at inference to obtain the encoder Jacobian $J_\Psi = \frac{\partial}{\partial x} \Psi(x;z)$. Consequently, we explored modern options in the \texttt{torch.func} namespace—including combinations of \texttt{vmap} with the forward and reverse functional Jacobian operators (\texttt{jacfwd}, \texttt{jacrev}) and the vector-Jacobian product (\texttt{vjp}). Our analysis, presented in the Supplementary Text, shows that simple two-point finite-difference schemes for the Jacobian compile and export cleanly, run quickly, and yield Jacobians whose accuracy is very close to the analytic solution.
In practice, we use an (absolute) step size $\delta_x = 5 \, 10^{-4}$ such that
\begin{equation}
    J(x;z) \approx \begin{bmatrix}
        \frac{\Psi(x+\delta_x \, e_1;z) - \Psi(x;z)}{\delta_x} & \dots &         \frac{\Psi(x+\delta_x \, e_j;z) - \Psi(x;z)}{\delta_x} & \dots & \frac{\Psi(x+\delta_x \, e_n;z) - \Psi(x;z)}{\delta_x}
    \end{bmatrix},
\end{equation}
where $e_j \in \mathbb{R}^n$ is the $j$th canonical basis vector in $x$-coordinates.
This allows us to exploit \emph{AOTInductor}, a specialized version of \emph{TorchInductor}, to export a compiled executable, which runs at up to \SI{15}{kHz} on the M4 Max CPU - a 9x increase over the standard eager inference mode.

In case the Jacobian of the encoder $J_\Psi(x;z)$ exhibits close-to-singular values, the numerical stability of the inference, can be improved by computing the inverse as $J_\Psi^{-1}(x;z) \equiv \left ( J_\Psi(x;z) + \epsilon_\mathrm{inv} \, \mathbb{I}_n \right )^{-1}$, where, for example, $\epsilon_\mathrm{inv} = 10^{-6}$.

\subsection{Online Shaping of the Learned Motion}
In order to improve the practicality of using the learned orbital motion primitives, we introduce in this section approaches that allow us to modulate the learned velocity field to adjust the task or modify its characteristics without having to retrain the \gls{OSMP}. 

First, we introduce variables that allow us to spatially translate and scale the learned velocity field
\begin{equation}
    \dot{x}(t) = \tilde{f}(x \, ;z) \coloneq s_\mathrm{f} \, f \left ( \frac{x(t)-x^\mathrm{o}}{s_\mathrm{f}}; z \right ).
\end{equation}
Here, $s_\mathrm{f} \in \mathbb{R}_{>0}$ controls the spatial scale of the velocity field. When $s_\mathrm{f} = 1$, the executed motion primitive is equal to the learned motion primitive. $x^\mathrm{o} \in \mathbb{R}^{n}$ defines the origin of the velocity field.

However, we are not limited to affine transformations such as translation and scaling. Additionally, we can adjust the period and convergence characteristics of the velocity field online. Specifically, by scaling the polar angular velocity $\omega(\varphi)$ with the factor $s_\omega \in \mathbb{R}_{>0}$, we can either slow-down ($0 < s_\omega < 1$) or speed-up ($s_\omega > 1$) the periodic motion.
Furthermore, the convergence of trajectories onto the $\mathbb{S}^1$ limit cycle can be made more or less aggressive by adjusting the convergence gain $k_\mathrm{conv} \in \mathbb{R}_{>0}$. Usually, we set $\alpha = \beta = k_\mathrm{conv} \, s_\omega$.

Finally, constraints in the oracle or actuation space (e.g., joint limits, environment obstacles) might pose challenges to the deployment of the orbital motion primitive in real-world settings when the system is initialized (far) off the oracle.
In these situations, we would not want to start our periodic motion directly, but instead, we would first converge into a neighborhood around the oracle that is collision-free. We devise a strategy that is able to accomplish such behavior by scaling the polar angular velocity $\tilde{\omega}$ as a function $\sigma: \mathbb{R}_{>0} \to \mathbb{R}$ of the distance from the limit cycle $d_\mathrm{lc}$
\begin{equation}\small
    d_\mathrm{lc} = \sqrt{\frac{\left (\sqrt{y_1^2 + y_2^2} - R \right)^2 + \sum_{i=2}^{n} y_i^2}{n-1}},
    \qquad
    \tilde{\omega} = \sigma(\varphi, d_\mathrm{lc}) = \exp \left ( - \frac{\max(d_\mathrm{lc} - R_\mathrm{sm}, 0)^2}{2 \, \sigma_\mathrm{sm}^2} \right ) \, \omega(\varphi),
\end{equation}
where $d_\mathrm{lc} \in \mathbb{R}_{>0}$ the Euclidean distance of the latent state $y$ from the limit cycle normalized by the DOF.
The mapping $d_\mathrm{lc} \mapsto \tilde{\omega}$ can be intuitively interpreted as follows: in a tube of radius $R_\mathrm{sm}$ around the limit cycle, we apply the nominal polar angular velocity $\omega(\varphi)$. Outside of that tube, we reduce the angular velocity using a Gaussian function with RMS width $\sigma_\mathrm{sm} \in \mathbb{R}_{>0}$. In the limit $d_\mathrm{lc} \to \infty$, the polar angular velocity is zero: $\lim_{d_\mathrm{lc} \to \infty} \sigma(d_\mathrm{lc}) = 0$.


\subsection{Phase Synchronization of Multiple Motion Primitives}
In many real-world applications, it is essential to synchronize the phases of multiple learned (orbital) motion primitives~\citep{gams2015accelerating}. For instance, in turtle swimming, the phases of the two limbs must align, while in (human) walking, the periodic movement of the two legs should be offset by $\pi$. To address this, we developed a controller that can synchronize the motion of two or more systems.
Here, we consider that we trained $n_\mathrm{s}$ \glspl{OSMP}. We refer to the latent state of the $i$th system, where $i \in \mathbb{N}_{n_\mathrm{s}}$, as ${}_{i} y$. The polar phase of each system is given by ${}_{i} \varphi = \operatorname{atan2}({}_{i} y_2, {}_{i} y_1)$. We then construct a symmetric matrix $\delta \Phi^* \in \mathbb{R}^{(n_\mathrm{s}-1) \times (n_\mathrm{s}-1)}$ that contains the desired phase offsets. For example, $\delta \Phi^*_{ij} = \delta \Phi^*_{ji} \in [-\pi, \pi)$ specifies the desired phase offset between the $i$th and the $j$th system. In the case of $\delta \Phi^* = 0^{(n_\mathrm{s}-1) \times (n_\mathrm{s}-1)}$, we ask the phase difference between all systems to be zero.
We then adopt a technique from the field of network synchronization~\citep{dorfler2014synchronization} that allows the alignment of the \glspl{OSMP} in phase. Namely, we define a feedback controller that acts on the angular velocity of the latent system
\begin{equation}
    {}_{i} \tilde{\omega}(\varphi) = {}_{i} \omega({}_{i}\varphi) \, \left (1  -k_\mathrm{ps} \sum_{j=1}^{n_\mathrm{s}} \sin \left ( \delta \Phi^*_{ij} + {}_{i} \varphi - {}_{j} \varphi \right ) \right ),
\end{equation}
where $\omega(\varphi), \tilde{\omega} \in \mathbb{R}$ are the default and modified polar angular velocities of the systems, respectively. 
$k_\mathrm{ps} \in \mathbb{R}_{>0}$ is the phase synchronization gain that determines how quickly the systems synchronize.